"""Document loading utilities used throughout the RAG service."""

from __future__ import annotations
import json
import os
import re
import fnmatch
import opik
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

from langchain_aws import ChatBedrock
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from src.service.rag_service.models import RawDocument, AllDocument
from src.service.rag_service.utils import Logger, Config

# ðŸ”¹ Opik tracing
from src.service.opik_tracing import trace_with_metadata
from opik import track, opik_context

SUPPORTED_TEXT_EXTENSIONS = {".txt", ".md", ".json", ".csv", ".log"}
logger = Logger.get_logger(__name__)


class CaseDocumentLoader:
    """Locate and read case resources generated by the extraction pipeline."""

    def __init__(
        self,
        resource_root: Path | None = None,
        allowed_extensions: Iterable[str] | None = None,
    ) -> None:

        if "__file__" in globals():
            base_dir = Path(__file__).resolve().parent.parent.parent.parent
        else:
            base_dir = Path(os.getcwd()).resolve().parent.parent.parent.parent

        path = f"{base_dir}/{resource_root}"
        default_root = f"{base_dir}/resources"
        self.resource_root = path if resource_root else default_root

        self.allowed_extensions = {
            ext.lower() if ext.startswith(".") else f".{ext.lower()}"
            for ext in (allowed_extensions or SUPPORTED_TEXT_EXTENSIONS)
        }

        if not Path(self.resource_root).exists():
            logger.error("Path not found")

    def resolve_case_dir(self, case_id: str) -> Path:
        case_id = case_id.strip()

        if not isinstance(self.resource_root, Path):
            self.resource_root = Path(self.resource_root).expanduser().resolve()

        case_dir = (self.resource_root / case_id).resolve()

        if not case_dir.exists() or not case_dir.is_dir():
            matches = list(self.resource_root.rglob(case_id))
            if matches:
                return matches[0]
            raise FileNotFoundError(
                f"Case directory not found for '{case_id}'. "
                f"Tried: {case_dir}. resource_root is: {self.resource_root}"
            )
        return case_dir

    def iter_output_files(self, case_id: str) -> List[tuple[Path, str]]:
        case_dir = self.resolve_case_dir(case_id)
        files: List[tuple[Path, str]] = []

        for document_dir in sorted(p for p in case_dir.iterdir() if p.is_dir()):
            output_dir = document_dir / "output"
            if not output_dir.exists():
                continue
            for file_path in sorted(output_dir.rglob("*")):
                if (
                    file_path.is_file()
                    and file_path.suffix.lower() in self.allowed_extensions
                ):
                    files.append((file_path, document_dir.name))

        final_dir = case_dir / "final_output"
        if final_dir.exists():
            for file_path in sorted(final_dir.rglob("*")):
                if (
                    file_path.is_file()
                    and file_path.suffix.lower() in self.allowed_extensions
                ):
                    files.append((file_path, "final_output"))

        if not files:
            raise FileNotFoundError(
                f"No supported text documents found under 'output' for case '{case_id}'."
            )
        return files


# =========================================================
# KPI INTENT DETECTION
# =========================================================

class KPIReferenceLoader:
    def __init__(self):
        self.FINAL_RE = re.compile(
            r"\b(final(\s*decision)?|decision|decline|approve|outcome)\b", re.I
        )
        self.KPI_RE = re.compile(
            r"\b(kpi|kpis|key\s*performance\s*indicator[s]?|metric|definition)\b", re.I
        )
        self.GREETING_RE = re.compile(
            r"^\s*(hi+|hello|hey|hiya|yo|thanks|thank\s*you|thank\s*u)\b.*$", re.I
        )
        self.config = Config()

    def heuristic_intents(self, q: str) -> Dict[str, bool]:
        logger.info("Using heuristic intent detection for question.")
        return {
            "decision_intent": bool(self.FINAL_RE.search(q or "")),
            "kpi_intent": bool(self.KPI_RE.search(q or "")),
        }

    # ðŸ”¥ FULLY TRACED LLM INTENT DETECTION
    @track(name="llm_intent_detection", capture_input=True, capture_output=True)
    def llm_intents(self, q: str) -> Dict[str, bool]:
        logger.info("Using LLM intent detection for question.")

        intent_system = """You classify user questions for retrieval augmentation.
Return ONLY JSON with two booleans:
{ "decision_intent": true, "kpi_intent": false }
Definitions:
- decision_intent: user asks about final decision, approval/decline, reason, outcome.
- kpi_intent: user asks about KPIs or KPI definitions/meaning/metrics.
No extra text.
"""

        intent_user = """User question:
{question}

Return JSON only.
"""

        intent_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", intent_system),
                ("user", intent_user),
            ]
        )

        with opik.start_as_current_span(name="llm_call") as span:
            llm_small = ChatBedrock(
                model_id="amazon.nova-micro-v1:0",
                region="us-east-1",
                aws_access_key_id=self.config.aws_access_key,
                aws_secret_access_key=self.config.aws_secret_key,
                temperature=0,
            )

            intent_chain = intent_prompt | llm_small | StrOutputParser()
            raw = intent_chain.invoke({"question": q})

            span.metadata({
                "llm_model": "amazon.nova-micro-v1:0",
                "region": "us-east-1",
                "response_length": len(raw),
            })

        try:
            data = json.loads(raw)
            result = {
                "decision_intent": bool(data.get("decision_intent", False)),
                "kpi_intent": bool(data.get("kpi_intent", False)),
            }

            span.metadata = {"intent_result": result}
            return result

        except Exception:
            return self.heuristic_intents(q)

    def detect_intents(self, question: str) -> Dict[str, bool]:
        if self.GREETING_RE.match(question or ""):
            return {"decision_intent": False, "kpi_intent": False}

        h = self.heuristic_intents(question)
        if h["decision_intent"] or h["kpi_intent"]:
            return h

        return self.llm_intents(question)
